import os
import random

import gymnasium as gym
import numpy as np
import torch
from rollout_buffer import RolloutBuffer

# MRBL big picture:
# 1. Start with a random policy
# 2. Sample trajectories using the random policy
# 3. Train a model using the sampled trajectories
# 4. Plan using the model in imagination. This means imagine multiple trajectories using the model and pick the best one that gives the highest reward.
# 5. Execute the first action of the best trajectory in the real environment.
# 6. Replan and add the new trajectory to the dataset.
# 7. Repeat from step 2.
""" There are two types of data here: one in imagination and one in real world. Our dynamics model learns from the data collected in
real world. The data in imagination is used to plan. The data in imagination is generated by sampling from the dynamics model+MPC.
The data in real world is generated by executing the first action of the best trajectory in the real environment.
"""


class RandomPolicy:
    def __init__(self, env):
        self.env = env

    def __call__(self, *args, **kwargs):
        return self.env.action_space.sample()


def seed_everything(seed: int, env: gym.Env):
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    env.action_space.seed(seed)


def main():
    env = gym.make("CartPole-v1", render_mode="human")
    policy = RandomPolicy(env)
    action = policy()
    buffer = RolloutBuffer()
    state = env.reset(seed=42)
    seed_everything(42, env)
    for _ in range(100):
        env.render()
        action = policy()
        next_state, reward, done, _, _ = env.step(action)
        buffer.push(state, action, reward, next_state, done)
        state = next_state
        if done:
            env.reset()
    print("Size of the buffer: ", len(buffer))
    os.makedirs("data", exist_ok=True)
    buffer.save("data/buffer.npy")
    env.close()


if __name__ == "__main__":
    main()
