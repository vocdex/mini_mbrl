# VQ-GAN
VQ-GAN is a direct extension of VQ-VAE with more modern tricks. It still follows the **two-stage** approach of VQ-VAE, but with a few key differences:
1. Stage 1:
    - While architectually similar to VQ-VAE, the loss function is different. It uses perceptual loss LPIPS instead of MSE that is baed on deep feature spaces, which is more aligned with human perception. This is done typically using a pre-trained network like VGG.
    - Patch-based GAN loss: it trains a discriminator to distinguish between real image patches and reconstructed patches generated by the VQGAN. This forces VQGAN decoder to produce much more realistic images and locally detailed images.
In turn, this allows VQGAN to achieve **much higher compression rates** while maintaining high quality.
2. Stage 2:
    - Instead of learning a new prior with PixelCNN, VQ-GAN uses a **Transformer** (GPT) to model the discrete latent space. This allows for better modeling of long-range dependencies and complex structures in the data.

VQ-VAE mostly focused on toy dataset such as CIFAR10 and ImageNet(128x128), but VQ-GAN's focus is on high-resolution image synthesis, such as 512x512 and 1024x1024 images.
